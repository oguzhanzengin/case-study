{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8bba00",
   "metadata": {
    "cellUniqueIdByVincent": "3c6ef"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m application_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(application_test_direction,)\n\u001b[0;32m     27\u001b[0m application_train_direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moğuzhan\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcase-study\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcase-study\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhome-credit-default-risk\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mapplication_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 28\u001b[0m application_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapplication_train_direction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m application_train\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m     31\u001b[0m application_train\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[1;32mc:\\Users\\oğuzhan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oğuzhan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oğuzhan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\oğuzhan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lofo import LOFOImportance, Dataset as LOFO_Dataset, plot_importance\n",
    "import optuna\n",
    "import shap\n",
    "\n",
    "application_test_direction=r\"C:\\Users\\oğuzhan\\Desktop\\case-study\\case-study\\home-credit-default-risk\\application_test.csv\"\n",
    "application_test = pd.read_csv(application_test_direction,)\n",
    "\n",
    "application_train_direction=r\"C:\\Users\\oğuzhan\\Desktop\\case-study\\case-study\\home-credit-default-risk\\application_train.csv\"\n",
    "application_train = pd.read_csv(application_train_direction,)\n",
    "\n",
    "application_train.head()\n",
    "application_train.info()\n",
    "application_train.isnull().sum().sort_values(ascending=False).head(20)\n",
    "application_train.describe()\n",
    "\n",
    "\"<class 'pandas.core.frame.DataFrame'>\"\n",
    "\"RangeIndex: 307511 entries, 0 to 307510\"\n",
    "\"Columns: 122 entries, SK_ID_CURR to AMT_REQ_CREDIT_BUREAU_YEAR\"\n",
    "\"dtypes: float64(65), int64(41), object(16)\"\n",
    "\"memory usage: 286.2+ MB\"\n",
    "\n",
    "missing = application_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (application_train.isnull().sum() / len(application_train) * 100).sort_values(ascending=False)\n",
    "\n",
    "missing_df = pd.DataFrame({'missing_count': missing, 'missing_percent': percent})\n",
    "missing_df.head(20)\n",
    "\n",
    "\"\"\"missing_count\tmissing_percent\n",
    "COMMONAREA_AVG\t214865\t69.872297\n",
    "COMMONAREA_MODE\t214865\t69.872297\n",
    "COMMONAREA_MEDI\t214865\t69.872297\n",
    "NONLIVINGAPARTMENTS_MEDI\t213514\t69.432963\n",
    "NONLIVINGAPARTMENTS_MODE\t213514\t69.432963\n",
    "NONLIVINGAPARTMENTS_AVG\t213514\t69.432963\n",
    "FONDKAPREMONT_MODE\t210295\t68.386172\n",
    "LIVINGAPARTMENTS_AVG\t210199\t68.354953\n",
    "LIVINGAPARTMENTS_MEDI\t210199\t68.354953\n",
    "LIVINGAPARTMENTS_MODE\t210199\t68.354953\n",
    "FLOORSMIN_MODE\t208642\t67.848630\n",
    "FLOORSMIN_AVG\t208642\t67.848630\n",
    "FLOORSMIN_MEDI\t208642\t67.848630\n",
    "YEARS_BUILD_AVG\t204488\t66.497784\n",
    "YEARS_BUILD_MODE\t204488\t66.497784\n",
    "YEARS_BUILD_MEDI\t204488\t66.497784\n",
    "OWN_CAR_AGE\t202929\t65.990810\n",
    "LANDAREA_MEDI\t182590\t59.376738\n",
    "LANDAREA_AVG\t182590\t59.376738\n",
    "LANDAREA_MODE\t182590\t59.376738\"\"\"\n",
    "\n",
    "cat_cols = application_train.select_dtypes(include=['object']).columns\n",
    "cat_cols\n",
    "\n",
    "\"\"\"Index(['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
    "       'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n",
    "       'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE',\n",
    "       'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE',\n",
    "       'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE'],\n",
    "      dtype='object')\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_le = application_train.copy()\n",
    "\n",
    "label_encoders = {}  # Sonradan inverse transform için gerekli\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_le[col] = df_le[col].astype(str)  # NaN ve kategoriler için güvenli\n",
    "    df_le[col] = le.fit_transform(df_le[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "df_le.head()\n",
    "\n",
    "\"\"\"\tSK_ID_CURR\tTARGET\tNAME_CONTRACT_TYPE\tCODE_GENDER\tFLAG_OWN_CAR\tFLAG_OWN_REALTY\tCNT_CHILDREN\tAMT_INCOME_TOTAL\tAMT_CREDIT\tAMT_ANNUITY\t...\tFLAG_DOCUMENT_18\tFLAG_DOCUMENT_19\tFLAG_DOCUMENT_20\tFLAG_DOCUMENT_21\tAMT_REQ_CREDIT_BUREAU_HOUR\tAMT_REQ_CREDIT_BUREAU_DAY\tAMT_REQ_CREDIT_BUREAU_WEEK\tAMT_REQ_CREDIT_BUREAU_MON\tAMT_REQ_CREDIT_BUREAU_QRT\tAMT_REQ_CREDIT_BUREAU_YEAR\n",
    "0\t100002\t1\t0\t1\t0\t1\t0\t202500.0\t406597.5\t24700.5\t...\t0\t0\t0\t0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\n",
    "1\t100003\t0\t0\t0\t0\t0\t0\t270000.0\t1293502.5\t35698.5\t...\t0\t0\t0\t0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "2\t100004\t0\t1\t1\t1\t1\t0\t67500.0\t135000.0\t6750.0\t...\t0\t0\t0\t0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "3\t100006\t0\t0\t0\t0\t1\t0\t135000.0\t312682.5\t29686.5\t...\t0\t0\t0\t0\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "4\t100007\t0\t0\t1\t0\t1\t0\t121500.0\t513000.0\t21865.5\t...\t0\t0\t0\t0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "5 rows × 122 columns\"\"\"\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "imputer = IterativeImputer(\n",
    "    estimator=BayesianRidge(),\n",
    "    max_iter=10,\n",
    "    initial_strategy='median',\n",
    "    imputation_order='ascending',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_imputed = imputer.fit_transform(df_le)\n",
    "\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df_le.columns)\n",
    "\n",
    "df_imputed.head()\n",
    "\n",
    "\"\"\"SK_ID_CURR\tTARGET\tNAME_CONTRACT_TYPE\tCODE_GENDER\tFLAG_OWN_CAR\tFLAG_OWN_REALTY\tCNT_CHILDREN\tAMT_INCOME_TOTAL\tAMT_CREDIT\tAMT_ANNUITY\t...\tFLAG_DOCUMENT_18\tFLAG_DOCUMENT_19\tFLAG_DOCUMENT_20\tFLAG_DOCUMENT_21\tAMT_REQ_CREDIT_BUREAU_HOUR\tAMT_REQ_CREDIT_BUREAU_DAY\tAMT_REQ_CREDIT_BUREAU_WEEK\tAMT_REQ_CREDIT_BUREAU_MON\tAMT_REQ_CREDIT_BUREAU_QRT\tAMT_REQ_CREDIT_BUREAU_YEAR\n",
    "0\t100002.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\t202500.0\t406597.5\t24700.5\t...\t0.0\t0.0\t0.0\t0.0\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t1.000000\n",
    "1\t100003.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t270000.0\t1293502.5\t35698.5\t...\t0.0\t0.0\t0.0\t0.0\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\n",
    "2\t100004.0\t0.0\t1.0\t1.0\t1.0\t1.0\t0.0\t67500.0\t135000.0\t6750.0\t...\t0.0\t0.0\t0.0\t0.0\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\n",
    "3\t100006.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t135000.0\t312682.5\t29686.5\t...\t0.0\t0.0\t0.0\t0.0\t0.004125\t0.005844\t0.039374\t0.197952\t0.277668\t2.153276\n",
    "4\t100007.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t121500.0\t513000.0\t21865.5\t...\t0.0\t0.0\t0.0\t0.0\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\n",
    "5 rows × 122 columns\"\"\"\n",
    "\n",
    "X = df_imputed.drop(columns=['TARGET'])\n",
    "y = df_imputed['TARGET']\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "\"((307511, 121), (307511,))\"\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(\"Fold:\", fold, \n",
    "          \"| Train size:\", len(train_idx), \n",
    "          \"| Validation size:\", len(val_idx))\n",
    "    \n",
    "\"\"\"Fold: 0 | Train size: 246008 | Validation size: 61503\n",
    "Fold: 1 | Train size: 246009 | Validation size: 61502\n",
    "Fold: 2 | Train size: 246009 | Validation size: 61502\n",
    "Fold: 3 | Train size: 246009 | Validation size: 61502\n",
    "Fold: 4 | Train size: 246009 | Validation size: 61502\"\"\"\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "cat_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=300,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        loss_function='Logloss',\n",
    "        verbose=False,\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "\n",
    "    preds = model.predict_proba(X_val)[:, 1]\n",
    "    score = roc_auc_score(y_val, preds)\n",
    "    cat_scores.append(score)\n",
    "    print(f\"Fold {fold} ROC-AUC: {score:.4f}\")\n",
    "\n",
    "print(\"\\nCatBoost Mean ROC-AUC:\", sum(cat_scores)/len(cat_scores))\n",
    "\n",
    "\"\"\"Fold 0 ROC-AUC: 0.9298\n",
    "Fold 1 ROC-AUC: 0.9333\n",
    "Fold 2 ROC-AUC: 0.9302\n",
    "Fold 3 ROC-AUC: 0.9297\n",
    "Fold 4 ROC-AUC: 0.9305\n",
    "\n",
    "CatBoost Mean ROC-AUC: 0.9307146242760815\"\"\"\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ridge_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "\n",
    "    model = RidgeClassifier()\n",
    "    model.fit(X_train_s, y_train)\n",
    "\n",
    "    preds = model.decision_function(X_val_s)\n",
    "    score = roc_auc_score(y_val, preds)\n",
    "    ridge_scores.append(score)\n",
    "    print(f\"Fold {fold} ROC-AUC: {score:.4f}\")\n",
    "\n",
    "print(\"\\nRidge Mean ROC-AUC:\", sum(ridge_scores)/len(ridge_scores))\n",
    "\n",
    "\"\"\"Fold 0 ROC-AUC: 0.7825\n",
    "Fold 1 ROC-AUC: 0.7930\n",
    "Fold 2 ROC-AUC: 0.7858\n",
    "Fold 3 ROC-AUC: 0.7900\n",
    "Fold 4 ROC-AUC: 0.7808\n",
    "\n",
    "Ridge Mean ROC-AUC: 0.7864047409455731\"\"\"\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "best_cat_model = CatBoostClassifier(\n",
    "    depth=8,\n",
    "    learning_rate=0.05,\n",
    "    iterations=1500,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "best_cat_model.fit(X, y)\n",
    "\n",
    "df_lofo = df_imputed.copy()  # TARGET dahil\n",
    "\n",
    "feature_names = df_lofo.columns.tolist()\n",
    "feature_names.remove(\"TARGET\")\n",
    "\n",
    "lofo_dataset = Dataset(\n",
    "    df_lofo,           # tek dataframe\n",
    "    \"TARGET\",          # target'ın kolon adı (string)\n",
    "    feature_names      # feature listesi\n",
    ")\n",
    "\n",
    "lofo_model = CatBoostClassifier(\n",
    "    depth=4,\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\",\n",
    "    verbose=False,\n",
    "    random_seed=42\n",
    ")\n",
    "importance_df = lofo.get_importance()\n",
    "\n",
    "plot_importance(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "7976d"
   },
   "outputs": [],
   "source": [
    "Below are the steps for two models, CatBoost and Sklearn Ridge, that we&#39;d like you to\n",
    "perform:\n",
    "1. Prepare the necessary preprocessing steps for both models, utilizing existing resources if\n",
    "available.\n",
    "2. Determine the appropriate validation strategy for model validation (e.g., KFold,\n",
    "StratifiedKFold).\n",
    "3. Provide initial prediction results with simple parameters for both models.\n",
    "4. Perform feature selection using lofo-importance as outlined in this article: [Link to the\n",
    "article].\n",
    "5. Implement hyperparameter optimization using techniques such as Grid Search, Random\n",
    "Search, or Bayesian Search. If possible, consider using Optuna (https://optuna.org/).\n",
    "6. Demonstrate how your choices from step 3 to step 5 have improved model performance,\n",
    "documenting the pros and cons of each experiment.\n",
    "7. Interpret model variables using SHAP values. You can use this resource.\n",
    "8. (Optional) Explore feature engineering techniques, creating new variables and validating\n",
    "their impact on model performance.\n",
    "You can access the dataset here.\n",
    "Finally, please compile your work into a Jupyter notebook with the last 7-8 headings &amp;\n",
    "presentation format. Feel free to reach out if you have any questions or need clarification.\n",
    "We are looking forward to seeing your progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5ee2c",
   "metadata": {
    "cellUniqueIdByVincent": "706ca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "7f231"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "5968d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "86580"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "61bc3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "54a7e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "61efd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "3655a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "40962"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "b9aee"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vincent": {
   "sessionId": "86188cd052ec9959812476fb_2025-11-30T14-52-41-273Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
